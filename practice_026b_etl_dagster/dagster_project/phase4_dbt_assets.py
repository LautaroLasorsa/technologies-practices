"""Phase 4: dbt Integration (~20 min)

dbt + DAGSTER -- THE BEST INTEGRATION IN THE MARKET:
    dbt (data build tool) is the standard for SQL-based data transformations.
    It lets you write SELECT statements and handles CREATE TABLE / incremental
    updates automatically. Think of it as "make for SQL."

    The challenge: dbt doesn't orchestrate itself. It needs something to:
    - Decide WHEN to run dbt models
    - Connect dbt models to UPSTREAM data sources (Python extract/load steps)
    - Connect dbt models to DOWNSTREAM consumers (dashboards, ML pipelines)
    - Track lineage across the entire pipeline (Python + SQL)

    In Airflow, dbt integration is crude:
        dbt_run = BashOperator(
            task_id="dbt_run",
            bash_command="dbt run --models my_model",
        )
        extract_task >> dbt_run >> downstream_task
        # No visibility into WHICH dbt models ran, their dependencies, or lineage

    In Dagster, dbt integration is native:
        @dbt_assets(manifest=dbt_manifest_path)
        def my_dbt_assets(context, dbt: DbtCliResource):
            yield from dbt.cli(["build"], context=context).stream()

        # Each dbt model becomes a SEPARATE Dagster asset with:
        # - Its own materialization history
        # - Dependencies on other dbt models (from ref())
        # - Dependencies on upstream Python assets
        # - Downstream Python assets can depend on specific dbt models
        # - Full lineage: Python -> dbt staging -> dbt mart -> Python consumer

HOW IT WORKS:
    1. dbt compiles a "manifest.json" that describes all models and their dependencies
    2. Dagster reads the manifest and creates one asset per dbt model
    3. dbt's ref('model_name') becomes a Dagster asset dependency
    4. When you materialize a dbt asset, Dagster calls `dbt run --select model_name`
    5. The asset graph shows Python assets and dbt models side by side

WHAT YOU'LL LEARN:
    1. Loading dbt models as Dagster assets with @dbt_assets
    2. Connecting upstream Python assets to dbt source tables
    3. Connecting downstream Python assets to dbt model outputs
    4. Unified lineage graph across Python and SQL
"""

from pathlib import Path
from typing import Any

from dagster import (
    AssetExecutionContext,
    AssetKey,
    MaterializeResult,
    MetadataValue,
    asset,
)

# ---------------------------------------------------------------------------
# dbt project paths
# ---------------------------------------------------------------------------
# These paths point to the dbt project inside the Docker container.
# The dbt_project/ directory is mounted at /app/dbt_project/.
# ---------------------------------------------------------------------------

DBT_PROJECT_DIR = Path("/app/dbt_project")
DBT_PROFILES_DIR = Path("/app/dbt_project")

# The manifest.json is generated by `dbt parse` or `dbt compile`.
# Our Docker entrypoint runs `dbt parse` on startup, creating this file.
DBT_MANIFEST_PATH = DBT_PROJECT_DIR / "target" / "manifest.json"


# ---------------------------------------------------------------------------
# REFERENCE IMPLEMENTATION: upstream Python asset that feeds dbt
# ---------------------------------------------------------------------------
# This asset writes data to a PostgreSQL table that dbt models read as a "source".
# It bridges the Python world (extract/load) with the SQL world (dbt transform).
# ---------------------------------------------------------------------------

from dagster_project.resources import PostgresResource


@asset(
    description="Load raw weather data into PostgreSQL table that dbt reads as a source. "
    "This bridges Python extraction with dbt SQL transformations.",
    group_name="phase4_dbt_pipeline",
    compute_kind="python",
    # This key_prefix makes the asset key "raw/weather_source".
    # We'll reference this in the dbt source definition.
    key_prefix=["raw"],
)
def weather_source(
    context: AssetExecutionContext,
    postgres: PostgresResource,
) -> MaterializeResult:
    """Ensure raw weather data exists in PostgreSQL for dbt to consume.

    COMPARISON WITH AIRFLOW:
        In Airflow, you'd have a PythonOperator that loads data, then a
        BashOperator that runs dbt. The connection between them is implicit
        (they both happen to use the same database table).

        In Dagster, the connection is EXPLICIT: this Python asset produces
        a table, and the dbt source declaration tells Dagster that dbt
        models depend on this table. The lineage is tracked automatically.
    """
    postgres.ensure_table(
        "raw_weather_readings",
        """
        id SERIAL PRIMARY KEY,
        city TEXT NOT NULL,
        temperature_c REAL NOT NULL,
        humidity_pct REAL NOT NULL,
        wind_speed_kmh REAL NOT NULL,
        reading_date DATE NOT NULL,
        recorded_at TIMESTAMP DEFAULT NOW()
        """,
    )

    rows = postgres.execute_query("SELECT COUNT(*) FROM raw_weather_readings")
    count = rows[0][0]

    context.log.info(f"Weather source table has {count} rows")

    return MaterializeResult(
        metadata={
            "row_count": MetadataValue.int(count or 0),
            "table": MetadataValue.text("raw_weather_readings"),
        }
    )


# ---------------------------------------------------------------------------
# TODO(human): Implement the @dbt_assets function
# ---------------------------------------------------------------------------
# YOUR TASK: Implement the dbt_weather_models function that loads dbt models
# as Dagster assets.
#
# WHAT THIS FUNCTION SHOULD DO:
#   1. Use the @dbt_assets decorator with the manifest path
#   2. Inside the function, call dbt.cli(["build"], context=context)
#   3. Yield the events from the dbt run using .stream()
#
# HOW @dbt_assets WORKS:
#   The @dbt_assets decorator reads the dbt manifest.json and creates one
#   Dagster asset for each dbt model. The decorator GENERATES the assets;
#   the function body defines HOW to materialize them (by running dbt).
#
#   Think of it as: the manifest tells Dagster WHAT assets exist and their
#   dependencies; the function tells Dagster HOW to create them.
#
#   @dbt_assets(manifest=DBT_MANIFEST_PATH)
#   def my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):
#       yield from dbt.cli(["build"], context=context).stream()
#
#   That's it! Each dbt model becomes an asset. dbt's ref() dependencies
#   become Dagster asset dependencies. The asset graph shows:
#       weather_source (Python) -> stg_weather_readings (dbt) -> daily_weather_summary (dbt)
#
# WHAT IS DbtCliResource?
#   A Dagster resource that wraps the dbt CLI. It's configured in definitions.py
#   with the project directory and profiles directory. When you call
#   dbt.cli(["build"], context=context), it runs `dbt build` for the specific
#   models that need materialization.
#
# WHAT DOES .stream() DO?
#   dbt.cli() returns a DbtCliInvocation object. Calling .stream() yields
#   Dagster events (AssetMaterialization, AssetObservation) as dbt processes
#   each model. This is how Dagster tracks which dbt models were built and
#   their metadata (row counts, execution time).
#
# IMPORTANT -- MANIFEST MUST EXIST:
#   The @dbt_assets decorator reads the manifest at MODULE LOAD TIME.
#   Our Docker entrypoint runs `dbt parse` to generate it. If the manifest
#   doesn't exist, Dagster will fail to load the code location.
#
#   For local development, you'd run: dbt parse --project-dir dbt_project
#
# STEP BY STEP:
#   1. Import DbtCliResource from dagster_dbt
#   2. Import dbt_assets from dagster_dbt
#   3. Decorate your function with @dbt_assets(manifest=DBT_MANIFEST_PATH)
#   4. The function takes (context: AssetExecutionContext, dbt: DbtCliResource)
#   5. Inside: yield from dbt.cli(["build"], context=context).stream()
#
# AFTER IMPLEMENTING:
#   1. Save this file
#   2. In Dagit, go to Assets and look for the dbt models:
#      - stg_weather_readings (staging model, fully implemented in SQL)
#      - daily_weather_summary (mart model, has a TODO for you to implement the SQL)
#   3. Notice how they appear in the SAME graph as Python assets
#   4. Materialize the full chain: weather_source -> stg_weather_readings -> daily_weather_summary -> summary_consumer
# ---------------------------------------------------------------------------

# Conditional import: only load dbt assets if the manifest exists.
# This allows the code to be importable even during initial setup
# when dbt parse hasn't been run yet.

_dbt_assets_loaded = False

try:
    from dagster_dbt import DbtCliResource, dbt_assets

    if DBT_MANIFEST_PATH.exists():
        @dbt_assets(manifest=DBT_MANIFEST_PATH)
        def dbt_weather_models(
            context: AssetExecutionContext,
            dbt: DbtCliResource,
        ):
            """Materialize dbt weather models as Dagster assets.

            TODO(human): This function currently has a stub implementation.
            Replace the stub with the real dbt CLI invocation.

            The real implementation is just ONE line:
                yield from dbt.cli(["build"], context=context).stream()

            That single line:
            - Runs `dbt build` for the selected models
            - Yields AssetMaterialization events for each model
            - Captures metadata (row counts, execution time, SQL compiled)
            - Updates the Dagster asset catalog

            Compare with Airflow where you'd need:
                BashOperator(
                    task_id="dbt_build",
                    bash_command="dbt build --select model_name",
                )
            And you'd get zero lineage, zero metadata, zero asset tracking.
            """
            # ----------------------------------------------------------
            # STUB: Replace with real implementation.
            # yield from dbt.cli(["build"], context=context).stream()
            # ----------------------------------------------------------
            context.log.warning(
                "dbt_weather_models is using stub -- replace with: "
                "yield from dbt.cli(['build'], context=context).stream()"
            )
            # Must yield at least nothing for the generator to work
            return
            yield  # noqa: unreachable -- makes this a generator function

        _dbt_assets_loaded = True
    else:
        import warnings
        warnings.warn(
            f"dbt manifest not found at {DBT_MANIFEST_PATH}. "
            "dbt assets will not be loaded. Run `dbt parse` to generate the manifest.",
            stacklevel=2,
        )

except ImportError:
    import warnings
    warnings.warn(
        "dagster-dbt not installed. Phase 4 dbt assets will not be available.",
        stacklevel=2,
    )


# ---------------------------------------------------------------------------
# Downstream Python asset that consumes dbt model output
# ---------------------------------------------------------------------------
# This demonstrates the full lineage: Python -> dbt -> Python.
# The asset depends on the dbt model output (a PostgreSQL table).
# ---------------------------------------------------------------------------


@asset(
    description="Consumes the dbt daily_weather_summary model and generates alerts "
    "for extreme weather conditions. Demonstrates Python -> dbt -> Python lineage.",
    group_name="phase4_dbt_pipeline",
    compute_kind="python",
    # This asset depends on the dbt model "daily_weather_summary".
    # The AssetKey must match the dbt model's asset key in the manifest.
    # By default, dbt models get keys like AssetKey(["model_name"]).
    deps=[AssetKey(["daily_weather_summary"])],
)
def weather_alerts(
    context: AssetExecutionContext,
    postgres: PostgresResource,
) -> MaterializeResult:
    """Generate weather alerts from dbt-transformed summary data.

    COMPARISON WITH AIRFLOW:
        In Airflow, this would be a PythonOperator downstream of the dbt
        BashOperator. The dependency is: extract >> dbt_run >> alert_task.
        But Airflow doesn't know WHICH dbt models the alert task depends on.
        If you add a new dbt model, Airflow still waits for ALL models to finish.

        In Dagster, this asset depends specifically on `daily_weather_summary`.
        If that specific dbt model is materialized, this asset can run --
        even if other dbt models are still processing. Fine-grained dependencies.

    This asset reads from the dbt model's output table and generates alerts
    for cities with extreme weather conditions.
    """
    # Read from the dbt model's output table
    rows = postgres.execute_query(
        """
        SELECT city, avg_temperature, avg_humidity, max_wind, reading_count, reading_date
        FROM daily_weather_summary
        WHERE avg_temperature > 35 OR avg_temperature < -20 OR max_wind > 80
        """
    )

    alerts = []
    for row in rows:
        city, avg_temp, avg_hum, max_wind, count, date = row
        reasons = []
        if avg_temp > 35:
            reasons.append(f"extreme heat ({avg_temp:.1f}C)")
        if avg_temp < -20:
            reasons.append(f"extreme cold ({avg_temp:.1f}C)")
        if max_wind > 80:
            reasons.append(f"high wind ({max_wind:.1f} km/h)")
        alerts.append({"city": city, "date": str(date), "reasons": reasons})

    context.log.info(f"Generated {len(alerts)} weather alerts")

    return MaterializeResult(
        metadata={
            "alert_count": MetadataValue.int(len(alerts)),
            "alerts_preview": MetadataValue.json(alerts[:5]),
        }
    )
