"""
Phase 2: Ingest articles into Qdrant.

YOU IMPLEMENT the core functions marked with TODO(human).

Concepts:
  - **PointStruct**: a point = (id, vector, payload). The payload is a dict of
    arbitrary JSON-serializable metadata attached to the vector.
  - **Upsert**: insert-or-update. If a point with the same ID exists, it's replaced.
  - **Batching**: uploading points in batches reduces HTTP round-trips. Qdrant's
    client.upsert() accepts a list of PointStruct -- send them in chunks.

Docs:
  - PointStruct: https://python-client.qdrant.tech/qdrant_client.models
  - client.upsert(): https://python-client.qdrant.tech/qdrant_client.qdrant_client

Run: python app/ingest.py
"""

import json
import time
from pathlib import Path

from qdrant_client import models

from client import create_client
from config import COLLECTION_NAME, DEFAULT_BATCH_SIZE

ARTICLES_PATH = Path(__file__).parent / "articles.json"


def load_articles(path: Path) -> list[dict]:
    """Load articles from the JSON file generated by generate_data.py."""
    return json.loads(path.read_text(encoding="utf-8"))


def article_to_point(article: dict) -> models.PointStruct:
    """Convert a single article dict into a Qdrant PointStruct.

    A PointStruct has three fields:
      - id: unique integer or UUID
      - vector: list of floats (must match collection's vector dimension)
      - payload: dict of metadata (any JSON-serializable fields)

    The vector is stored separately from the payload. During search, Qdrant
    compares query vectors against stored vectors, then returns matching
    payloads.
    """
    return models.PointStruct(
        id=article["id"],
        vector=article["vector"],
        payload={
            "title": article["title"],
            "category": article["category"],
            "year": article["year"],
            "word_count": article["word_count"],
        },
    )


def upsert_batch(client, articles: list[dict], batch_size: int = DEFAULT_BATCH_SIZE) -> int:
    """Upload articles to Qdrant in batches.

    TODO(human): Implement this function.

    Steps:
      1. Iterate over `articles` in chunks of `batch_size`
         (Hint: use range(0, len(articles), batch_size) to get chunk starts)
      2. For each chunk, convert articles to PointStruct using article_to_point()
      3. Call client.upsert(collection_name=COLLECTION_NAME, points=<list of PointStruct>)
      4. Return the total number of points upserted

    Why batching matters:
      - Each upsert() call is an HTTP request to Qdrant
      - 500 individual requests vs 8 batched requests (batch_size=64) is ~60x fewer round-trips
      - Qdrant also processes batches more efficiently internally (bulk WAL writes)

    Docs: https://python-client.qdrant.tech/qdrant_client.qdrant_client#QdrantClient.upsert
    """
    total_upserted = 0
    # TODO(human): implement batch upsert loop here
    raise NotImplementedError("Implement upsert_batch()")
    return total_upserted


def upsert_single(client, article: dict) -> None:
    """Upload a single article to Qdrant.

    TODO(human): Implement this function.

    Steps:
      1. Convert the article to a PointStruct using article_to_point()
      2. Call client.upsert() with a list containing that single point

    This is useful for real-time updates (e.g., a new article is published and
    you want it searchable immediately). For bulk loading, use upsert_batch().

    Docs: https://python-client.qdrant.tech/qdrant_client.qdrant_client#QdrantClient.upsert
    """
    # TODO(human): implement single-point upsert here
    raise NotImplementedError("Implement upsert_single()")


def verify_ingestion(client) -> None:
    """Check the collection's point count to confirm ingestion succeeded."""
    info = client.get_collection(COLLECTION_NAME)
    print(f"  Collection '{COLLECTION_NAME}' now has {info.points_count} points")


def main() -> None:
    print("=== Phase 2: Ingesting articles into Qdrant ===\n")

    articles = load_articles(ARTICLES_PATH)
    print(f"  Loaded {len(articles)} articles from {ARTICLES_PATH}")

    client = create_client()

    # --- Batch upsert (main ingestion path) ---
    print(f"\n[1/2] Batch upserting {len(articles)} articles (batch_size={DEFAULT_BATCH_SIZE})...")
    start = time.perf_counter()
    count = upsert_batch(client, articles)
    elapsed = time.perf_counter() - start
    print(f"  Upserted {count} points in {elapsed:.3f}s ({count / elapsed:.0f} points/sec)")

    # --- Single upsert (demonstrate real-time update) ---
    print("\n[2/2] Upserting a single extra article (simulating real-time update)...")
    extra_article = {
        "id": 9999,
        "title": "Hot-off-the-press: New Vector Index Breakthrough",
        "category": "AI",
        "year": 2025,
        "word_count": 3200,
        "vector": [0.0] * 128,  # zero vector as placeholder
    }
    upsert_single(client, extra_article)
    print(f"  Upserted article id={extra_article['id']}")

    # --- Verify ---
    print("\n[Verify]")
    verify_ingestion(client)

    print("\nIngestion complete. Next: run 'python app/search.py'")


if __name__ == "__main__":
    main()
